# DockerÁõëÊéß‰∏éÊó•Âøó

## üéØ Â≠¶‰π†ÁõÆÊ†á

- Ê∑±ÂÖ•ÁêÜËß£DockerÁõëÊéß‰ΩìÁ≥ªÂíåÊó•ÂøóÁÆ°ÁêÜ
- ÊéåÊè°Prometheus„ÄÅGrafanaÁ≠âÁõëÊéßÂ∑•ÂÖ∑ÁöÑ‰ΩøÁî®
- Â≠¶‰ºöËÆæËÆ°È´òÊïàÁöÑÊó•ÂøóÊî∂ÈõÜÂíåÂàÜÊûêÁ≥ªÁªü
- ‰∫ÜËß£ÂëäË≠¶Êú∫Âà∂ÂíåÊïÖÈöúÊéíÈô§ÊµÅÁ®ã

## üìö ÁõëÊéß‰∏éÊó•ÂøóÊ¶ÇËø∞

### 1. ÁõëÊéß‰ΩìÁ≥ªÊû∂ÊûÑ

```javascript
// DockerÁõëÊéß‰ΩìÁ≥ª
const dockerMonitoring = {
  metrics_layers: {
    infrastructure: {
      description: 'Âü∫Á°ÄËÆæÊñΩÂ±ÇÁõëÊéß',
      targets: ['CPU‰ΩøÁî®Áéá', 'ÂÜÖÂ≠ò‰ΩøÁî®Áéá', 'Á£ÅÁõòI/O', 'ÁΩëÁªúÊµÅÈáè'],
      tools: ['node-exporter', 'cadvisor', 'docker stats']
    },
    
    container: {
      description: 'ÂÆπÂô®Â±ÇÁõëÊéß',
      targets: ['ÂÆπÂô®Áä∂ÊÄÅ', 'ËµÑÊ∫êÈôêÂà∂', 'ÈáçÂêØÊ¨°Êï∞', 'ÂÅ•Â∫∑Ê£ÄÊü•'],
      tools: ['docker metrics', 'cadvisor', 'container-exporter']
    },
    
    application: {
      description: 'Â∫îÁî®Â±ÇÁõëÊéß',
      targets: ['ÂìçÂ∫îÊó∂Èó¥', 'ÂêûÂêêÈáè', 'ÈîôËØØÁéá', '‰∏öÂä°ÊåáÊ†á'],
      tools: ['Â∫îÁî®metrics', 'APMÂ∑•ÂÖ∑', 'Ëá™ÂÆö‰πâÁõëÊéß']
    },
    
    business: {
      description: '‰∏öÂä°Â±ÇÁõëÊéß',
      targets: ['Áî®Êà∑Ë°å‰∏∫', '‰∏öÂä°ÊµÅÁ®ã', 'SLAÊåáÊ†á', 'KPIÁõëÊéß'],
      tools: ['‰∏öÂä°ÁõëÊéßÁ≥ªÁªü', 'Êï∞ÊçÆÂàÜÊûêÂπ≥Âè∞']
    }
  },
  
  logging_types: {
    system_logs: 'Á≥ªÁªüÊó•Âøó - syslog, kern.log',
    container_logs: 'ÂÆπÂô®Êó•Âøó - stdout/stderr',
    application_logs: 'Â∫îÁî®Êó•Âøó - ‰∏öÂä°Êó•Âøó',
    audit_logs: 'ÂÆ°ËÆ°Êó•Âøó - ÂÆâÂÖ®ÂíåÂêàËßÑ'
  },
  
  observability_pillars: [
    'Metrics - ÊåáÊ†áÁõëÊéß',
    'Logging - Êó•ÂøóËÆ∞ÂΩï', 
    'Tracing - ÈìæË∑ØËøΩË∏™'
  ]
};

console.log('DockerÁõëÊéß‰ΩìÁ≥ª:', dockerMonitoring);
```

### 2. ÁõëÊéßÊåáÊ†á‰ΩìÁ≥ª

```yaml
# monitoring-metrics.yml
metrics_categories:
  system_metrics:
    cpu:
      - cpu_usage_percent
      - cpu_load_average
      - cpu_context_switches
    memory:
      - memory_usage_bytes
      - memory_available_bytes
      - memory_cached_bytes
      - memory_swap_usage
    disk:
      - disk_usage_percent
      - disk_io_read_bytes
      - disk_io_write_bytes
      - disk_io_operations
    network:
      - network_receive_bytes
      - network_transmit_bytes
      - network_receive_packets
      - network_transmit_packets

  container_metrics:
    resources:
      - container_cpu_usage_percent
      - container_memory_usage_bytes
      - container_memory_limit_bytes
      - container_blkio_read_bytes
      - container_blkio_write_bytes
    status:
      - container_up
      - container_restart_count
      - container_exit_code
      - container_start_time

  application_metrics:
    performance:
      - http_request_duration_seconds
      - http_requests_total
      - http_request_size_bytes
      - http_response_size_bytes
    errors:
      - error_rate
      - error_count_by_type
      - timeout_count
    business:
      - user_login_count
      - order_count
      - payment_success_rate
```

## üìä PrometheusÁõëÊéßÁ≥ªÁªü

### 1. PrometheusÈÖçÁΩÆ

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'docker-swarm'
    environment: 'production'

# ÂëäË≠¶ËßÑÂàôÊñá‰ª∂
rule_files:
  - "alert_rules.yml"
  - "recording_rules.yml"

# ÂëäË≠¶ÁÆ°ÁêÜÂô®ÈÖçÁΩÆ
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

# ÁõëÊéßÁõÆÊ†áÈÖçÁΩÆ
scrape_configs:
  # PrometheusËá™ÁõëÊéß
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Node Exporter - Á≥ªÁªüÊåáÊ†á
  - job_name: 'node-exporter'
    static_configs:
      - targets: 
        - 'node-exporter:9100'
    scrape_interval: 10s
    metrics_path: /metrics

  # cAdvisor - ÂÆπÂô®ÊåáÊ†á
  - job_name: 'cadvisor'
    static_configs:
      - targets:
        - 'cadvisor:8080'
    scrape_interval: 10s
    metrics_path: /metrics

  # Docker daemon metrics
  - job_name: 'docker'
    static_configs:
      - targets:
        - 'docker-host:9323'
    scrape_interval: 10s

  # Â∫îÁî®ÊåáÊ†á - ÊúçÂä°ÂèëÁé∞
  - job_name: 'myapp'
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        port: 3000
    relabel_configs:
      - source_labels: [__meta_docker_container_label_com_docker_swarm_service_name]
        regex: myapp-web
        action: keep
      - source_labels: [__meta_docker_container_label_prometheus_scrape]
        regex: true
        action: keep
      - source_labels: [__meta_docker_port_private]
        regex: 3000
        action: keep

  # Êï∞ÊçÆÂ∫ìÁõëÊéß
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  # RedisÁõëÊéß  
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  # NginxÁõëÊéß
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']

# ËøúÁ®ãÂÜôÂÖ•ÈÖçÁΩÆÔºàÂèØÈÄâÔºâ
remote_write:
  - url: "https://prometheus-remote-write.example.com/api/v1/write"
    basic_auth:
      username: "user"
      password: "password"
```

### 2. ÂëäË≠¶ËßÑÂàôÈÖçÁΩÆ

```yaml
# alert_rules.yml
groups:
  - name: container_alerts
    rules:
      # ÂÆπÂô®ÂÅúÊ≠¢ËøêË°å
      - alert: ContainerDown
        expr: up{job="cadvisor"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Container is down"
          description: "Container {{ $labels.instance }} has been down for more than 1 minute."

      # ÂÆπÂô®CPU‰ΩøÁî®ÁéáËøáÈ´ò
      - alert: ContainerHighCPU
        expr: rate(container_cpu_usage_seconds_total[5m]) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage in container"
          description: "Container {{ $labels.name }} CPU usage is above 80% for 5 minutes."

      # ÂÆπÂô®ÂÜÖÂ≠ò‰ΩøÁî®ÁéáËøáÈ´ò
      - alert: ContainerHighMemory
        expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) * 100 > 90
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage in container"
          description: "Container {{ $labels.name }} memory usage is above 90%."

      # ÂÆπÂô®ÈáçÂêØÈ¢ëÁπÅ
      - alert: ContainerRestartTooMuch
        expr: increase(container_start_time_seconds[1h]) > 5
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Container restarting too much"
          description: "Container {{ $labels.name }} has restarted more than 5 times in the last hour."

  - name: application_alerts
    rules:
      # HTTPÈîôËØØÁéáËøáÈ´ò
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100 > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate"
          description: "Error rate is above 5% for 5 minutes."

      # ÂìçÂ∫îÊó∂Èó¥ËøáÈïø
      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency"
          description: "95th percentile latency is above 1 second."

      # Êï∞ÊçÆÂ∫ìËøûÊé•Â§±Ë¥•
      - alert: DatabaseDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Database is down"
          description: "PostgreSQL database has been down for more than 1 minute."

  - name: system_alerts
    rules:
      # Á≥ªÁªüË¥üËΩΩËøáÈ´ò
      - alert: HighSystemLoad
        expr: node_load1 > node_machine_info{cores="2"} * 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High system load"
          description: "System load is above 80% of CPU cores."

      # Á£ÅÁõòÁ©∫Èó¥‰∏çË∂≥
      - alert: DiskSpaceLow
        expr: (1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Disk space low"
          description: "Disk space usage is above 85% on {{ $labels.device }}."

      # ÂÜÖÂ≠ò‰ΩøÁî®ÁéáËøáÈ´ò
      - alert: HighMemoryUsage
        expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage"
          description: "Memory usage is above 90%."
```

### 3. Grafana‰ª™Ë°®Êùø

```json
{
  "dashboard": {
    "id": null,
    "title": "Docker Container Monitoring",
    "tags": ["docker", "containers"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Container CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(container_cpu_usage_seconds_total{name!=\"\"}[5m]) * 100",
            "legendFormat": "{{ name }}"
          }
        ],
        "yAxes": [
          {
            "label": "Percent",
            "max": 100,
            "min": 0
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 0
        }
      },
      {
        "id": 2,
        "title": "Container Memory Usage", 
        "type": "graph",
        "targets": [
          {
            "expr": "container_memory_usage_bytes{name!=\"\"}",
            "legendFormat": "{{ name }}"
          }
        ],
        "yAxes": [
          {
            "label": "Bytes"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 0
        }
      },
      {
        "id": 3,
        "title": "HTTP Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{ method }} {{ status }}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 8
        }
      },
      {
        "id": 4,
        "title": "HTTP Response Time",
        "type": "graph", 
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          },
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "99th percentile"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 8
        }
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

## üìù Êó•ÂøóÁÆ°ÁêÜÁ≥ªÁªü

### 1. ELK StackÈÖçÁΩÆ

```yaml
# elk-stack.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    networks:
      - logging

  logstash:
    image: docker.elastic.co/logstash/logstash:8.5.0
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./logstash/config:/usr/share/logstash/config:ro
    ports:
      - "5044:5044"
      - "5000:5000"
    environment:
      - "LS_JAVA_OPTS=-Xms512m -Xmx512m"
    depends_on:
      - elasticsearch
    networks:
      - logging

  kibana:
    image: docker.elastic.co/kibana/kibana:8.5.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - logging

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.5.0
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: filebeat -e -strict.perms=false
    depends_on:
      - logstash
    deploy:
      mode: global
    networks:
      - logging

volumes:
  elasticsearch_data:

networks:
  logging:
    external: true
```

### 2. LogstashÈÖçÁΩÆ

```ruby
# logstash/pipeline/logstash.conf
input {
  beats {
    port => 5044
  }
  
  # Áõ¥Êé•‰ªéDocker APIÊî∂ÈõÜÊó•Âøó
  http {
    port => 5000
    codec => json
  }
}

filter {
  # Â§ÑÁêÜÊù•Ëá™FilebeatÁöÑDockerÊó•Âøó
  if [fields][log_type] == "docker" {
    # Ëß£ÊûêDockerÊó•ÂøóÂÖÉÊï∞ÊçÆ
    json {
      source => "message"
      target => "docker"
    }
    
    # ÊèêÂèñÂÆπÂô®‰ø°ÊÅØ
    if [container][name] {
      mutate {
        add_field => { "container_name" => "%{[container][name]}" }
      }
    }
    
    # Ëß£ÊûêÊó∂Èó¥Êà≥
    date {
      match => [ "[docker][time]", "ISO8601" ]
      target => "@timestamp"
    }
    
    # Â∫îÁî®Êó•ÂøóËß£Êûê
    if [container_name] =~ /myapp/ {
      # Ëß£ÊûêÂ∫îÁî®JSONÊó•Âøó
      if [docker][log] =~ /^\{/ {
        json {
          source => "[docker][log]"
          target => "app"
        }
        
        # ÊèêÂèñÊó•ÂøóÁ∫ßÂà´
        if [app][level] {
          mutate {
            add_field => { "log_level" => "%{[app][level]}" }
          }
        }
        
        # ÊèêÂèñËØ∑Ê±ÇID
        if [app][request_id] {
          mutate {
            add_field => { "request_id" => "%{[app][request_id]}" }
          }
        }
      }
    }
    
    # NginxÊó•ÂøóËß£Êûê
    if [container_name] =~ /nginx/ {
      grok {
        match => { "[docker][log]" => "%{COMBINEDAPACHELOG}" }
      }
      
      # Ëß£ÊûêÂìçÂ∫îÊó∂Èó¥
      if [response] {
        mutate {
          convert => { "response" => "integer" }
        }
      }
    }
  }
  
  # Ê∑ªÂä†ÁéØÂ¢ÉÊ†áÁ≠æ
  mutate {
    add_field => { "environment" => "production" }
    add_field => { "cluster" => "docker-swarm" }
  }
  
  # ÁßªÈô§‰∏çÈúÄË¶ÅÁöÑÂ≠óÊÆµ
  mutate {
    remove_field => [ "agent", "ecs", "host", "input" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "docker-logs-%{+YYYY.MM.dd}"
  }
  
  # ËæìÂá∫ÈîôËØØÊó•ÂøóÂà∞‰∏ìÈó®ÁöÑÁ¥¢Âºï
  if [log_level] == "error" or [response] >= 400 {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "docker-errors-%{+YYYY.MM.dd}"
    }
  }
  
  # Ë∞ÉËØïËæìÂá∫
  stdout { 
    codec => rubydebug 
  }
}
```

### 3. FilebeatÈÖçÁΩÆ

```yaml
# filebeat/filebeat.yml
filebeat.inputs:
  # DockerÂÆπÂô®Êó•Âøó
  - type: container
    paths:
      - /var/lib/docker/containers/*/*.log
    processors:
      - add_docker_metadata:
          host: "unix:///var/run/docker.sock"
      - decode_json_fields:
          fields: ["message"]
          target: ""
          overwrite_keys: true
    fields:
      log_type: docker
    fields_under_root: true

  # Á≥ªÁªüÊó•Âøó
  - type: log
    paths:
      - /var/log/syslog
      - /var/log/auth.log
    fields:
      log_type: system
    fields_under_root: true

# Â§ÑÁêÜÂô®ÈÖçÁΩÆ
processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_docker_metadata: ~
  - add_kubernetes_metadata: ~

# ËæìÂá∫ÈÖçÁΩÆ
output.logstash:
  hosts: ["logstash:5044"]

# Êó•ÂøóÁ∫ßÂà´
logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
```

## üö® ÂëäË≠¶ÁÆ°ÁêÜÁ≥ªÁªü

### 1. AlertmanagerÈÖçÁΩÆ

```yaml
# alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: 'app_password'

# Ë∑ØÁî±ÈÖçÁΩÆ
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default'
  routes:
    # ‰∏•ÈáçÂëäË≠¶
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 10s
      repeat_interval: 5m
    
    # Â∫îÁî®ÂëäË≠¶
    - match:
        service: myapp
      receiver: 'app-team'
    
    # Âü∫Á°ÄËÆæÊñΩÂëäË≠¶
    - match_re:
        alertname: ^(ContainerDown|HighSystemLoad|DiskSpaceLow)$
      receiver: 'infrastructure-team'

# Êé•Êî∂Âô®ÈÖçÁΩÆ
receivers:
  # ÈªòËÆ§Êé•Êî∂Âô®
  - name: 'default'
    email_configs:
      - to: 'devops@example.com'
        subject: '[ALERT] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Severity: {{ .Labels.severity }}
          {{ end }}

  # ‰∏•ÈáçÂëäË≠¶Êé•Êî∂Âô®
  - name: 'critical-alerts'
    email_configs:
      - to: 'oncall@example.com'
        subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
        body: |
          CRITICAL ALERT!
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Time: {{ .StartsAt }}
          {{ end }}
    
    # SlackÈÄöÁü•
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#alerts'
        title: 'Critical Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Instance:* {{ .Labels.instance }}
          *Severity:* {{ .Labels.severity }}
          {{ end }}
    
    # ‰ºÅ‰∏öÂæÆ‰ø°ÈÄöÁü•
    wechat_configs:
      - api_url: 'https://qyapi.weixin.qq.com/cgi-bin/webhook/send'
        corp_id: 'your_corp_id'
        api_secret: 'your_api_secret'
        to_user: '@all'
        message: |
          ‰∏•ÈáçÂëäË≠¶ÈÄöÁü•
          {{ range .Alerts }}
          ÂëäË≠¶: {{ .Annotations.summary }}
          ÊèèËø∞: {{ .Annotations.description }}
          ÂÆû‰æã: {{ .Labels.instance }}
          {{ end }}

  # Â∫îÁî®Âõ¢ÈòüÊé•Êî∂Âô®
  - name: 'app-team'
    email_configs:
      - to: 'app-team@example.com'
        subject: '[APP] {{ .GroupLabels.alertname }}'

  # Âü∫Á°ÄËÆæÊñΩÂõ¢ÈòüÊé•Êî∂Âô®
  - name: 'infrastructure-team'
    email_configs:
      - to: 'infra-team@example.com'
        subject: '[INFRA] {{ .GroupLabels.alertname }}'

# ÊäëÂà∂ËßÑÂàô
inhibit_rules:
  # Â¶ÇÊûúÂÆû‰æãÂÆïÊú∫ÔºåÊäëÂà∂ËØ•ÂÆû‰æãÁöÑÂÖ∂‰ªñÂëäË≠¶
  - source_match:
      alertname: 'InstanceDown'
    target_match_re:
      instance: '.*'
    equal: ['instance']
  
  # Â¶ÇÊûúÊúâ‰∏•ÈáçÂëäË≠¶ÔºåÊäëÂà∂Âêå‰∏ÄÊúçÂä°ÁöÑË≠¶ÂëäÂëäË≠¶
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['service']
```

### 2. Ëá™ÂÆö‰πâÂëäË≠¶ËÑöÊú¨

```bash
#!/bin/bash
# alert-webhook.sh - WebhookÂëäË≠¶Â§ÑÁêÜËÑöÊú¨

# Êé•Êî∂AlertmanagerÁöÑwebhookÊï∞ÊçÆ
read -r ALERT_DATA

# Ëß£ÊûêÂëäË≠¶Êï∞ÊçÆ
ALERT_NAME=$(echo "$ALERT_DATA" | jq -r '.alerts[0].labels.alertname')
SEVERITY=$(echo "$ALERT_DATA" | jq -r '.alerts[0].labels.severity')
INSTANCE=$(echo "$ALERT_DATA" | jq -r '.alerts[0].labels.instance')
DESCRIPTION=$(echo "$ALERT_DATA" | jq -r '.alerts[0].annotations.description')
STATUS=$(echo "$ALERT_DATA" | jq -r '.status')

echo "Êî∂Âà∞ÂëäË≠¶: $ALERT_NAME ($SEVERITY) - $INSTANCE"

# Ê†πÊçÆÂëäË≠¶Á±ªÂûãÊâßË°åÁõ∏Â∫îÂä®‰Ωú
case $ALERT_NAME in
    "ContainerDown")
        echo "ÂÆπÂô®‰∏ãÁ∫øÂëäË≠¶ÔºåÂ∞ùËØïÈáçÂêØ..."
        # Ëá™Âä®ÈáçÂêØÂÆπÂô®
        CONTAINER_NAME=$(echo "$INSTANCE" | cut -d: -f1)
        docker service update --force $CONTAINER_NAME
        ;;
        
    "HighMemoryUsage")
        echo "ÂÜÖÂ≠ò‰ΩøÁî®ÁéáËøáÈ´òÔºåÊî∂ÈõÜËØäÊñ≠‰ø°ÊÅØ..."
        # Êî∂ÈõÜÁ≥ªÁªü‰ø°ÊÅØ
        docker stats --no-stream > /tmp/container-stats-$(date +%Y%m%d-%H%M%S).log
        ;;
        
    "DiskSpaceLow")
        echo "Á£ÅÁõòÁ©∫Èó¥‰∏çË∂≥ÔºåÊ∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂..."
        # Ê∏ÖÁêÜDockerÁ≥ªÁªü
        docker system prune -f
        ;;
        
    *)
        echo "Êú™Áü•ÂëäË≠¶Á±ªÂûã: $ALERT_NAME"
        ;;
esac

# ËÆ∞ÂΩïÂëäË≠¶Âà∞Êó•Âøó
echo "$(date): $STATUS - $ALERT_NAME - $DESCRIPTION" >> /var/log/alerts.log

# ÂèëÈÄÅÂà∞Ëá™ÂÆö‰πâÈÄöÁü•Á≥ªÁªü
curl -X POST "http://notification-service/webhook" \
    -H "Content-Type: application/json" \
    -d "{
        \"alert\": \"$ALERT_NAME\",
        \"severity\": \"$SEVERITY\",
        \"instance\": \"$INSTANCE\",
        \"description\": \"$DESCRIPTION\",
        \"status\": \"$STATUS\",
        \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"
    }"
```

## üìä Â∫îÁî®ÊÄßËÉΩÁõëÊéß(APM)

### 1. Â∫îÁî®ÊåáÊ†áÊî∂ÈõÜ

```javascript
// metrics.js - Node.jsÂ∫îÁî®ÊåáÊ†áÊî∂ÈõÜ
const promClient = require('prom-client');
const express = require('express');

// ÂàõÂª∫ÊåáÊ†áÊ≥®ÂÜåË°®
const register = new promClient.Registry();

// Á≥ªÁªüÊåáÊ†á
promClient.collectDefaultMetrics({ register });

// HTTPËØ∑Ê±ÇÊåáÊ†á
const httpRequestsTotal = new promClient.Counter({
  name: 'http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status_code'],
  registers: [register]
});

const httpRequestDuration = new promClient.Histogram({
  name: 'http_request_duration_seconds',
  help: 'HTTP request duration in seconds',
  labelNames: ['method', 'route', 'status_code'],
  buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10],
  registers: [register]
});

// ‰∏öÂä°ÊåáÊ†á
const userRegistrations = new promClient.Counter({
  name: 'user_registrations_total',
  help: 'Total number of user registrations',
  registers: [register]
});

const activeUsers = new promClient.Gauge({
  name: 'active_users',
  help: 'Number of currently active users',
  registers: [register]
});

const orderValue = new promClient.Summary({
  name: 'order_value_dollars',
  help: 'Order value in dollars',
  percentiles: [0.5, 0.9, 0.95, 0.99],
  registers: [register]
});

// Êï∞ÊçÆÂ∫ìËøûÊé•Ê±†ÊåáÊ†á
const dbConnectionsActive = new promClient.Gauge({
  name: 'database_connections_active',
  help: 'Number of active database connections',
  registers: [register]
});

const dbQueryDuration = new promClient.Histogram({
  name: 'database_query_duration_seconds',
  help: 'Database query duration',
  labelNames: ['query_type', 'table'],
  buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5],
  registers: [register]
});

// ‰∏≠Èó¥‰ª∂ÔºöËÆ∞ÂΩïHTTPÊåáÊ†á
const metricsMiddleware = (req, res, next) => {
  const start = Date.now();
  
  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    const route = req.route ? req.route.path : req.path;
    
    httpRequestsTotal.inc({
      method: req.method,
      route: route,
      status_code: res.statusCode
    });
    
    httpRequestDuration.observe({
      method: req.method,
      route: route,
      status_code: res.statusCode
    }, duration);
  });
  
  next();
};

// Êï∞ÊçÆÂ∫ìÊü•ËØ¢ÂåÖË£ÖÂô®
const instrumentedQuery = async (query, params, queryType, table) => {
  const start = Date.now();
  
  try {
    const result = await db.query(query, params);
    const duration = (Date.now() - start) / 1000;
    
    dbQueryDuration.observe({
      query_type: queryType,
      table: table
    }, duration);
    
    return result;
  } catch (error) {
    dbQueryDuration.observe({
      query_type: queryType,
      table: table
    }, (Date.now() - start) / 1000);
    throw error;
  }
};

// Êö¥Èú≤ÊåáÊ†áÁ´ØÁÇπ
const app = express();
app.use(metricsMiddleware);

app.get('/metrics', async (req, res) => {
  try {
    // Êõ¥Êñ∞Âä®ÊÄÅÊåáÊ†á
    updateDynamicMetrics();
    
    res.set('Content-Type', register.contentType);
    res.end(await register.metrics());
  } catch (error) {
    res.status(500).end(error);
  }
});

// Êõ¥Êñ∞Âä®ÊÄÅÊåáÊ†á
function updateDynamicMetrics() {
  // Êõ¥Êñ∞Ê¥ªË∑ÉÁî®Êà∑Êï∞
  const activeUserCount = getActiveUserCount();
  activeUsers.set(activeUserCount);
  
  // Êõ¥Êñ∞Êï∞ÊçÆÂ∫ìËøûÊé•Êï∞
  const dbConnections = getDbConnectionCount();
  dbConnectionsActive.set(dbConnections);
}

module.exports = {
  register,
  httpRequestsTotal,
  httpRequestDuration,
  userRegistrations,
  activeUsers,
  orderValue,
  instrumentedQuery,
  metricsMiddleware
};
```

### 2. ÂàÜÂ∏ÉÂºèÈìæË∑ØËøΩË∏™

```javascript
// tracing.js - ÂàÜÂ∏ÉÂºèÈìæË∑ØËøΩË∏™
const { NodeSDK } = require('@opentelemetry/sdk-node');
const { Resource } = require('@opentelemetry/resources');
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');
const { JaegerExporter } = require('@opentelemetry/exporter-jaeger');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');

// ÈÖçÁΩÆËµÑÊ∫ê‰ø°ÊÅØ
const resource = new Resource({
  [SemanticResourceAttributes.SERVICE_NAME]: 'myapp',
  [SemanticResourceAttributes.SERVICE_VERSION]: process.env.APP_VERSION || '1.0.0',
  [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.NODE_ENV || 'development',
});

// ÈÖçÁΩÆJaegerÂØºÂá∫Âô®
const jaegerExporter = new JaegerExporter({
  endpoint: process.env.JAEGER_ENDPOINT || 'http://jaeger:14268/api/traces',
});

// ÂàùÂßãÂåñSDK
const sdk = new NodeSDK({
  resource: resource,
  traceExporter: jaegerExporter,
  instrumentations: [
    getNodeAutoInstrumentations({
      // Á¶ÅÁî®Êüê‰∫õËá™Âä®Âåñinstrument
      '@opentelemetry/instrumentation-fs': {
        enabled: false,
      },
    }),
  ],
});

// ÂêØÂä®ËøΩË∏™
sdk.start();

// ÊâãÂä®ÂàõÂª∫spanÁöÑÂ∑•ÂÖ∑ÂáΩÊï∞
const opentelemetry = require('@opentelemetry/api');

function createSpan(name, fn) {
  const tracer = opentelemetry.trace.getTracer('myapp');
  
  return tracer.startActiveSpan(name, async (span) => {
    try {
      const result = await fn(span);
      span.setStatus({ code: opentelemetry.SpanStatusCode.OK });
      return result;
    } catch (error) {
      span.recordException(error);
      span.setStatus({
        code: opentelemetry.SpanStatusCode.ERROR,
        message: error.message,
      });
      throw error;
    } finally {
      span.end();
    }
  });
}

module.exports = {
  createSpan,
  tracer: opentelemetry.trace.getTracer('myapp')
};
```

## üîß ÁõëÊéßËá™Âä®ÂåñËÑöÊú¨

### 1. ÂÅ•Â∫∑Ê£ÄÊü•ËÑöÊú¨

```bash
#!/bin/bash
# health-check-monitor.sh

SERVICES=("myapp-web" "myapp-db" "myapp-cache")
SLACK_WEBHOOK_URL="https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
CHECK_INTERVAL=60

echo "üè• ÂêØÂä®ÂÅ•Â∫∑Ê£ÄÊü•ÁõëÊéß..."

while true; do
    echo "$(date): ÊâßË°åÂÅ•Â∫∑Ê£ÄÊü•..."
    
    for service in "${SERVICES[@]}"; do
        echo "Ê£ÄÊü•ÊúçÂä°: $service"
        
        # Ê£ÄÊü•ÊúçÂä°Áä∂ÊÄÅ
        service_status=$(docker service ps $service --format "{{.CurrentState}}" --filter "desired-state=running" | head -1)
        
        if [[ $service_status =~ ^Running ]]; then
            echo "‚úÖ $service ËøêË°åÊ≠£Â∏∏"
            
            # È¢ùÂ§ñÁöÑÂÅ•Â∫∑Ê£ÄÊü•
            if [[ $service == "myapp-web" ]]; then
                # HTTPÂÅ•Â∫∑Ê£ÄÊü•
                if ! curl -f -s --max-time 10 http://localhost:3000/health >/dev/null; then
                    send_alert "‚ö†Ô∏è $service HTTPÂÅ•Â∫∑Ê£ÄÊü•Â§±Ë¥•"
                fi
            elif [[ $service == "myapp-db" ]]; then
                # Êï∞ÊçÆÂ∫ìËøûÊé•Ê£ÄÊü•
                if ! docker exec $(docker ps -q -f name=$service) pg_isready -U app >/dev/null 2>&1; then
                    send_alert "‚ö†Ô∏è $service Êï∞ÊçÆÂ∫ìËøûÊé•Ê£ÄÊü•Â§±Ë¥•"
                fi
            fi
            
        else
            send_alert "‚ùå $service Áä∂ÊÄÅÂºÇÂ∏∏: $service_status"
        fi
    done
    
    sleep $CHECK_INTERVAL
done

send_alert() {
    local message="$1"
    echo "$message"
    
    # ÂèëÈÄÅÂà∞Slack
    if [ ! -z "$SLACK_WEBHOOK_URL" ]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"$message\"}" \
            $SLACK_WEBHOOK_URL
    fi
    
    # ËÆ∞ÂΩïÂà∞Á≥ªÁªüÊó•Âøó
    logger "$message"
}
```

### 2. ÊÄßËÉΩÁõëÊéßËÑöÊú¨

```bash
#!/bin/bash
# performance-monitor.sh

THRESHOLD_CPU=80
THRESHOLD_MEMORY=85
THRESHOLD_DISK=90
REPORT_INTERVAL=300  # 5ÂàÜÈíü

echo "üìä ÂêØÂä®ÊÄßËÉΩÁõëÊéß..."

while true; do
    echo "$(date): Êî∂ÈõÜÊÄßËÉΩÊåáÊ†á..."
    
    # CPU‰ΩøÁî®Áéá
    cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1 | tr -d ' us,')
    cpu_usage=${cpu_usage%.*}  # ÂèñÊï¥Êï∞ÈÉ®ÂàÜ
    
    if [ "$cpu_usage" -gt "$THRESHOLD_CPU" ]; then
        echo "‚ö†Ô∏è CPU‰ΩøÁî®ÁéáËøáÈ´ò: ${cpu_usage}%"
        
        # ÊâæÂá∫CPU‰ΩøÁî®ÁéáÊúÄÈ´òÁöÑÂÆπÂô®
        echo "CPU‰ΩøÁî®ÁéáÊúÄÈ´òÁöÑÂÆπÂô®:"
        docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}" | sort -k2 -rn | head -5
    fi
    
    # ÂÜÖÂ≠ò‰ΩøÁî®Áéá
    memory_info=$(free | grep Mem)
    total_memory=$(echo $memory_info | awk '{print $2}')
    used_memory=$(echo $memory_info | awk '{print $3}')
    memory_usage=$(( used_memory * 100 / total_memory ))
    
    if [ "$memory_usage" -gt "$THRESHOLD_MEMORY" ]; then
        echo "‚ö†Ô∏è ÂÜÖÂ≠ò‰ΩøÁî®ÁéáËøáÈ´ò: ${memory_usage}%"
        
        # ÊâæÂá∫ÂÜÖÂ≠ò‰ΩøÁî®ÊúÄÈ´òÁöÑÂÆπÂô®
        echo "ÂÜÖÂ≠ò‰ΩøÁî®ÊúÄÈ´òÁöÑÂÆπÂô®:"
        docker stats --no-stream --format "table {{.Container}}\t{{.MemUsage}}" | sort -k2 -rn | head -5
    fi
    
    # Á£ÅÁõò‰ΩøÁî®Áéá
    while read output; do
        usage=$(echo $output | awk '{print $5}' | cut -d'%' -f1)
        partition=$(echo $output | awk '{print $6}')
        
        if [ "$usage" -gt "$THRESHOLD_DISK" ]; then
            echo "‚ö†Ô∏è Á£ÅÁõò‰ΩøÁî®ÁéáËøáÈ´ò: $partition ${usage}%"
            
            # ÊâæÂá∫Â§ßÊñá‰ª∂
            echo "ÊúÄÂ§ßÁöÑÊñá‰ª∂:"
            find $partition -type f -size +100M -exec ls -lh {} \; 2>/dev/null | sort -k5 -rh | head -5
        fi
    done < <(df -h | grep -vE '^Filesystem|tmpfs|cdrom')
    
    # DockerÁ≥ªÁªü‰ø°ÊÅØ
    echo ""
    echo "üê≥ DockerÁ≥ªÁªü‰ø°ÊÅØ:"
    docker system df
    
    # ÂÆπÂô®ËµÑÊ∫ê‰ΩøÁî®TOP 5
    echo ""
    echo "üìã ËµÑÊ∫ê‰ΩøÁî®TOP 5:"
    docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}" | sort -k2 -rn | head -6
    
    sleep $REPORT_INTERVAL
done
```

## üìù ‰∏ã‰∏ÄÊ≠•

ÊÅ≠ÂñúÔºÅÊÇ®Â∑≤ÁªèÂÆåÊàê‰∫ÜDockerÂ≠¶‰π†ÁöÑÂÖ®ÈÉ®12‰∏™Á´†ËäÇ„ÄÇÁé∞Âú®ÊÇ®ÂèØ‰ª•Ôºö

1. **ÂÆûË∑µÈ°πÁõÆ** - ‰ΩøÁî®Â≠¶Âà∞ÁöÑÁü•ËØÜÊûÑÂª∫ÂÆåÊï¥ÁöÑÂÆπÂô®ÂåñÂ∫îÁî®
2. **Ê∑±ÂÖ•Â≠¶‰π†** - Êé¢Á¥¢Kubernetes„ÄÅÊúçÂä°ÁΩëÊ†ºÁ≠âÈ´òÁ∫ßÂÆπÂô®ÊäÄÊúØ
3. **ËÆ§ËØÅËÄÉËØï** - ËÄÉËôëÂèÇÂä†DockerËÆ§ËØÅËÄÉËØïÈ™åËØÅÊÇ®ÁöÑÊäÄËÉΩ

## üéØ Êú¨Á´†Ë¶ÅÁÇπ

- ‚úÖ ÁêÜËß£DockerÁõëÊéß‰ΩìÁ≥ªÂíåÊó•ÂøóÁÆ°ÁêÜÊû∂ÊûÑ
- ‚úÖ ÊéåÊè°Prometheus„ÄÅGrafanaÁõëÊéßÂ∑•ÂÖ∑ÁöÑ‰ΩøÁî®
- ‚úÖ Â≠¶‰ºöELK StackÊó•ÂøóÊî∂ÈõÜÂíåÂàÜÊûêÁ≥ªÁªü
- ‚úÖ ‰∫ÜËß£ÂëäË≠¶Êú∫Âà∂ÂíåËá™Âä®ÂåñËøêÁª¥ËÑöÊú¨
- ‚úÖ ÊéåÊè°Â∫îÁî®ÊÄßËÉΩÁõëÊéßÂíåÂàÜÂ∏ÉÂºèËøΩË∏™

üéâ **ÊÅ≠ÂñúÊÇ®ÂÆåÊàê‰∫ÜÂÆåÊï¥ÁöÑDockerÂ≠¶‰π†‰πãÊóÖÔºÅ**

ÊÇ®Áé∞Âú®Â∑≤ÁªèÂÖ∑Â§á‰∫ÜÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠ÈÉ®ÁΩ≤„ÄÅÁÆ°ÁêÜÂíåÁõëÊéßDockerÂ∫îÁî®ÁöÑÂÖ®Èù¢ÊäÄËÉΩ„ÄÇÁªßÁª≠ÂÆûË∑µÂíåÊé¢Á¥¢ÔºåÊàê‰∏∫ÂÆπÂô®ÂåñÊäÄÊúØ‰∏ìÂÆ∂ÔºÅüê≥
