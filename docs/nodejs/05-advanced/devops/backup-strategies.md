# å¤‡ä»½ç­–ç•¥

## ğŸ“‹ æ¦‚è¿°

å¤‡ä»½ç­–ç•¥æ˜¯ä¿æŠ¤æ•°æ®å’Œç¡®ä¿ä¸šåŠ¡è¿ç»­æ€§çš„å…³é”®æªæ–½ã€‚æœ‰æ•ˆçš„å¤‡ä»½ç­–ç•¥åº”è¯¥åŒ…æ‹¬æ•°æ®å¤‡ä»½ã€ç³»ç»Ÿå¤‡ä»½ã€é…ç½®å¤‡ä»½å’Œç¾éš¾æ¢å¤è®¡åˆ’ï¼Œç¡®ä¿åœ¨å„ç§æ•…éšœåœºæ™¯ä¸‹éƒ½èƒ½å¿«é€Ÿæ¢å¤æœåŠ¡ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£å¤‡ä»½ç­–ç•¥çš„æ ¸å¿ƒæ¦‚å¿µå’Œé‡è¦æ€§
- æŒæ¡ä¸åŒç±»å‹æ•°æ®çš„å¤‡ä»½æ–¹æ³•
- å­¦ä¼šè®¾è®¡å’Œå®æ–½å¤‡ä»½è®¡åˆ’
- äº†è§£ç¾éš¾æ¢å¤å’Œä¸šåŠ¡è¿ç»­æ€§è§„åˆ’

## ğŸ“š å¤‡ä»½åŸºç¡€æ¦‚å¿µ

### å¤‡ä»½ç±»å‹

```mermaid
graph TB
    A[å¤‡ä»½ç±»å‹] --> B[å®Œå…¨å¤‡ä»½]
    A --> C[å¢é‡å¤‡ä»½]
    A --> D[å·®å¼‚å¤‡ä»½]
    
    B --> B1[å¤‡ä»½æ‰€æœ‰æ•°æ®]
    B --> B2[æ¢å¤é€Ÿåº¦å¿«]
    B --> B3[å­˜å‚¨ç©ºé—´å¤§]
    
    C --> C1[åªå¤‡ä»½å˜æ›´æ•°æ®]
    C --> C2[å­˜å‚¨ç©ºé—´å°]
    C --> C3[æ¢å¤æ—¶é—´é•¿]
    
    D --> D1[å¤‡ä»½è‡ªä¸Šæ¬¡å®Œå…¨å¤‡ä»½åçš„å˜æ›´]
    D --> D2[å¹³è¡¡å­˜å‚¨å’Œæ¢å¤æ—¶é—´]
```

### RTOå’ŒRPOæ¦‚å¿µ

```javascript
// æ¢å¤æ—¶é—´ç›®æ ‡ (RTO) - Recovery Time Objective
const RTO = {
  tier1: '15åˆ†é’Ÿ',    // å…³é”®ä¸šåŠ¡ç³»ç»Ÿ
  tier2: '4å°æ—¶',     // é‡è¦ä¸šåŠ¡ç³»ç»Ÿ
  tier3: '24å°æ—¶'     // ä¸€èˆ¬ä¸šåŠ¡ç³»ç»Ÿ
};

// æ¢å¤ç‚¹ç›®æ ‡ (RPO) - Recovery Point Objective
const RPO = {
  tier1: '5åˆ†é’Ÿ',     // å¯æ¥å—çš„æœ€å¤§æ•°æ®ä¸¢å¤±æ—¶é—´
  tier2: '1å°æ—¶',
  tier3: '24å°æ—¶'
};
```

## ğŸ›  æ•°æ®åº“å¤‡ä»½ç­–ç•¥

### PostgreSQLå¤‡ä»½

```bash
#!/bin/bash
# postgres-backup.sh

set -e

# é…ç½®å˜é‡
DB_HOST=${DB_HOST:-localhost}
DB_PORT=${DB_PORT:-5432}
DB_NAME=${DB_NAME:-nodejs_app}
DB_USER=${DB_USER:-postgres}
BACKUP_DIR=${BACKUP_DIR:-/backups/postgres}
RETENTION_DAYS=${RETENTION_DAYS:-30}
S3_BUCKET=${S3_BUCKET:-""}

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR"

# ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
BACKUP_FILE="$BACKUP_DIR/${DB_NAME}_${TIMESTAMP}.sql"
BACKUP_FILE_COMPRESSED="${BACKUP_FILE}.gz"

echo "ğŸ”„ å¼€å§‹æ•°æ®åº“å¤‡ä»½: $DB_NAME"

# æ‰§è¡Œå¤‡ä»½
pg_dump \
  --host="$DB_HOST" \
  --port="$DB_PORT" \
  --username="$DB_USER" \
  --dbname="$DB_NAME" \
  --verbose \
  --clean \
  --if-exists \
  --create \
  --format=plain \
  --file="$BACKUP_FILE"

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
echo "ğŸ“¦ å‹ç¼©å¤‡ä»½æ–‡ä»¶..."
gzip "$BACKUP_FILE"

# éªŒè¯å¤‡ä»½æ–‡ä»¶
if [ ! -f "$BACKUP_FILE_COMPRESSED" ]; then
    echo "âŒ å¤‡ä»½å¤±è´¥: æ–‡ä»¶ä¸å­˜åœ¨"
    exit 1
fi

BACKUP_SIZE=$(du -h "$BACKUP_FILE_COMPRESSED" | cut -f1)
echo "âœ… å¤‡ä»½å®Œæˆ: $BACKUP_FILE_COMPRESSED ($BACKUP_SIZE)"

# ä¸Šä¼ åˆ°S3ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
if [ -n "$S3_BUCKET" ]; then
    echo "â˜ï¸ ä¸Šä¼ å¤‡ä»½åˆ°S3..."
    aws s3 cp "$BACKUP_FILE_COMPRESSED" "s3://$S3_BUCKET/postgres/$(basename $BACKUP_FILE_COMPRESSED)"
    echo "âœ… S3ä¸Šä¼ å®Œæˆ"
fi

# æ¸…ç†æ—§å¤‡ä»½
echo "ğŸ§¹ æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶..."
find "$BACKUP_DIR" -name "${DB_NAME}_*.sql.gz" -mtime +$RETENTION_DAYS -delete
echo "âœ… æ¸…ç†å®Œæˆ"

# å¤‡ä»½éªŒè¯
echo "ğŸ” éªŒè¯å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§..."
if gunzip -t "$BACKUP_FILE_COMPRESSED"; then
    echo "âœ… å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡"
else
    echo "âŒ å¤‡ä»½æ–‡ä»¶æŸå"
    exit 1
fi

echo "ğŸ‰ å¤‡ä»½æµç¨‹å®Œæˆ"
```

### MongoDBå¤‡ä»½

```bash
#!/bin/bash
# mongodb-backup.sh

set -e

# é…ç½®å˜é‡
MONGO_HOST=${MONGO_HOST:-localhost}
MONGO_PORT=${MONGO_PORT:-27017}
MONGO_DB=${MONGO_DB:-nodejs_app}
MONGO_USER=${MONGO_USER:-""}
MONGO_PASS=${MONGO_PASS:-""}
BACKUP_DIR=${BACKUP_DIR:-/backups/mongodb}
RETENTION_DAYS=${RETENTION_DAYS:-30}

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR"

# ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
BACKUP_PATH="$BACKUP_DIR/${MONGO_DB}_${TIMESTAMP}"

echo "ğŸ”„ å¼€å§‹MongoDBå¤‡ä»½: $MONGO_DB"

# æ„å»ºmongodumpå‘½ä»¤
MONGODUMP_CMD="mongodump --host $MONGO_HOST:$MONGO_PORT --db $MONGO_DB --out $BACKUP_PATH"

if [ -n "$MONGO_USER" ] && [ -n "$MONGO_PASS" ]; then
    MONGODUMP_CMD="$MONGODUMP_CMD --username $MONGO_USER --password $MONGO_PASS"
fi

# æ‰§è¡Œå¤‡ä»½
eval $MONGODUMP_CMD

# å‹ç¼©å¤‡ä»½
echo "ğŸ“¦ å‹ç¼©å¤‡ä»½æ–‡ä»¶..."
tar -czf "${BACKUP_PATH}.tar.gz" -C "$BACKUP_DIR" "$(basename $BACKUP_PATH)"
rm -rf "$BACKUP_PATH"

BACKUP_SIZE=$(du -h "${BACKUP_PATH}.tar.gz" | cut -f1)
echo "âœ… å¤‡ä»½å®Œæˆ: ${BACKUP_PATH}.tar.gz ($BACKUP_SIZE)"

# æ¸…ç†æ—§å¤‡ä»½
find "$BACKUP_DIR" -name "${MONGO_DB}_*.tar.gz" -mtime +$RETENTION_DAYS -delete

echo "ğŸ‰ MongoDBå¤‡ä»½å®Œæˆ"
```

### Rediså¤‡ä»½

```bash
#!/bin/bash
# redis-backup.sh

set -e

# é…ç½®å˜é‡
REDIS_HOST=${REDIS_HOST:-localhost}
REDIS_PORT=${REDIS_PORT:-6379}
REDIS_PASSWORD=${REDIS_PASSWORD:-""}
BACKUP_DIR=${BACKUP_DIR:-/backups/redis}
RETENTION_DAYS=${RETENTION_DAYS:-7}

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR"

# ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
BACKUP_FILE="$BACKUP_DIR/redis_${TIMESTAMP}.rdb"

echo "ğŸ”„ å¼€å§‹Rediså¤‡ä»½"

# æ„å»ºredis-cliå‘½ä»¤
REDIS_CLI_CMD="redis-cli -h $REDIS_HOST -p $REDIS_PORT"

if [ -n "$REDIS_PASSWORD" ]; then
    REDIS_CLI_CMD="$REDIS_CLI_CMD -a $REDIS_PASSWORD"
fi

# æ‰§è¡ŒBGSAVEå‘½ä»¤
echo "ğŸ“¦ æ‰§è¡Œåå°ä¿å­˜..."
$REDIS_CLI_CMD BGSAVE

# ç­‰å¾…å¤‡ä»½å®Œæˆ
while [ "$($REDIS_CLI_CMD LASTSAVE)" = "$($REDIS_CLI_CMD LASTSAVE)" ]; do
    sleep 1
done

# å¤åˆ¶RDBæ–‡ä»¶
REDIS_DATA_DIR=$($REDIS_CLI_CMD CONFIG GET dir | tail -1)
REDIS_RDB_FILE=$($REDIS_CLI_CMD CONFIG GET dbfilename | tail -1)

cp "$REDIS_DATA_DIR/$REDIS_RDB_FILE" "$BACKUP_FILE"

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
gzip "$BACKUP_FILE"

BACKUP_SIZE=$(du -h "${BACKUP_FILE}.gz" | cut -f1)
echo "âœ… Rediså¤‡ä»½å®Œæˆ: ${BACKUP_FILE}.gz ($BACKUP_SIZE)"

# æ¸…ç†æ—§å¤‡ä»½
find "$BACKUP_DIR" -name "redis_*.rdb.gz" -mtime +$RETENTION_DAYS -delete

echo "ğŸ‰ Rediså¤‡ä»½æµç¨‹å®Œæˆ"
```

## ğŸ“ æ–‡ä»¶ç³»ç»Ÿå¤‡ä»½

### åº”ç”¨ä»£ç å’Œé…ç½®å¤‡ä»½

```bash
#!/bin/bash
# application-backup.sh

set -e

# é…ç½®å˜é‡
APP_DIR=${APP_DIR:-/opt/nodejs-app}
CONFIG_DIR=${CONFIG_DIR:-/etc/nodejs-app}
BACKUP_DIR=${BACKUP_DIR:-/backups/application}
RETENTION_DAYS=${RETENTION_DAYS:-30}
S3_BUCKET=${S3_BUCKET:-""}

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR"

# ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
BACKUP_FILE="$BACKUP_DIR/app_backup_${TIMESTAMP}.tar.gz"

echo "ğŸ”„ å¼€å§‹åº”ç”¨å¤‡ä»½"

# åˆ›å»ºå¤‡ä»½
tar -czf "$BACKUP_FILE" \
    --exclude="$APP_DIR/node_modules" \
    --exclude="$APP_DIR/logs" \
    --exclude="$APP_DIR/tmp" \
    --exclude="$APP_DIR/.git" \
    "$APP_DIR" \
    "$CONFIG_DIR"

BACKUP_SIZE=$(du -h "$BACKUP_FILE" | cut -f1)
echo "âœ… åº”ç”¨å¤‡ä»½å®Œæˆ: $BACKUP_FILE ($BACKUP_SIZE)"

# ä¸Šä¼ åˆ°S3
if [ -n "$S3_BUCKET" ]; then
    aws s3 cp "$BACKUP_FILE" "s3://$S3_BUCKET/application/$(basename $BACKUP_FILE)"
    echo "â˜ï¸ å·²ä¸Šä¼ åˆ°S3"
fi

# æ¸…ç†æ—§å¤‡ä»½
find "$BACKUP_DIR" -name "app_backup_*.tar.gz" -mtime +$RETENTION_DAYS -delete

echo "ğŸ‰ åº”ç”¨å¤‡ä»½æµç¨‹å®Œæˆ"
```

### ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶å¤‡ä»½

```bash
#!/bin/bash
# uploads-backup.sh

set -e

# é…ç½®å˜é‡
UPLOADS_DIR=${UPLOADS_DIR:-/var/www/uploads}
BACKUP_DIR=${BACKUP_DIR:-/backups/uploads}
S3_BUCKET=${S3_BUCKET:-""}
RETENTION_DAYS=${RETENTION_DAYS:-90}

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR"

echo "ğŸ”„ å¼€å§‹ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶å¤‡ä»½"

# ä½¿ç”¨rsyncè¿›è¡Œå¢é‡å¤‡ä»½
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
BACKUP_PATH="$BACKUP_DIR/uploads_$TIMESTAMP"

rsync -av \
    --delete \
    --exclude="*.tmp" \
    --exclude="*.temp" \
    "$UPLOADS_DIR/" \
    "$BACKUP_PATH/"

# å‹ç¼©å¤‡ä»½
tar -czf "${BACKUP_PATH}.tar.gz" -C "$BACKUP_DIR" "$(basename $BACKUP_PATH)"
rm -rf "$BACKUP_PATH"

BACKUP_SIZE=$(du -h "${BACKUP_PATH}.tar.gz" | cut -f1)
echo "âœ… ç”¨æˆ·æ–‡ä»¶å¤‡ä»½å®Œæˆ: ${BACKUP_PATH}.tar.gz ($BACKUP_SIZE)"

# åŒæ­¥åˆ°S3ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
if [ -n "$S3_BUCKET" ]; then
    aws s3 sync "$UPLOADS_DIR/" "s3://$S3_BUCKET/uploads/" \
        --delete \
        --exclude "*.tmp" \
        --exclude "*.temp"
    echo "â˜ï¸ å·²åŒæ­¥åˆ°S3"
fi

# æ¸…ç†æ—§å¤‡ä»½
find "$BACKUP_DIR" -name "uploads_*.tar.gz" -mtime +$RETENTION_DAYS -delete

echo "ğŸ‰ ç”¨æˆ·æ–‡ä»¶å¤‡ä»½å®Œæˆ"
```

## ğŸ”„ è‡ªåŠ¨åŒ–å¤‡ä»½ç³»ç»Ÿ

### å¤‡ä»½è°ƒåº¦å™¨

```javascript
// backup-scheduler.js
const cron = require('node-cron');
const { exec } = require('child_process');
const fs = require('fs').promises;
const path = require('path');
const logger = require('./logger');

class BackupScheduler {
  constructor(config) {
    this.config = config;
    this.backupJobs = new Map();
  }

  // å¯åŠ¨å¤‡ä»½è°ƒåº¦
  start() {
    logger.info('Starting backup scheduler');
    
    // æ•°æ®åº“å¤‡ä»½ - æ¯å¤©å‡Œæ™¨2ç‚¹
    this.scheduleJob('database-backup', '0 2 * * *', () => {
      this.runDatabaseBackup();
    });
    
    // åº”ç”¨å¤‡ä»½ - æ¯å¤©å‡Œæ™¨3ç‚¹
    this.scheduleJob('application-backup', '0 3 * * *', () => {
      this.runApplicationBackup();
    });
    
    // ç”¨æˆ·æ–‡ä»¶å¤‡ä»½ - æ¯6å°æ—¶
    this.scheduleJob('uploads-backup', '0 */6 * * *', () => {
      this.runUploadsBackup();
    });
    
    // æ—¥å¿—å¤‡ä»½ - æ¯å¤©å‡Œæ™¨4ç‚¹
    this.scheduleJob('logs-backup', '0 4 * * *', () => {
      this.runLogsBackup();
    });
    
    // å¤‡ä»½éªŒè¯ - æ¯å¤©ä¸Šåˆ9ç‚¹
    this.scheduleJob('backup-verification', '0 9 * * *', () => {
      this.verifyBackups();
    });
  }

  scheduleJob(name, schedule, task) {
    const job = cron.schedule(schedule, async () => {
      try {
        logger.info(`Starting scheduled backup: ${name}`);
        await task();
        logger.info(`Completed scheduled backup: ${name}`);
      } catch (error) {
        logger.error(`Backup failed: ${name}`, { error: error.message });
        await this.sendAlertNotification(name, error);
      }
    }, {
      scheduled: false,
      timezone: this.config.timezone || 'UTC'
    });
    
    this.backupJobs.set(name, job);
    job.start();
    
    logger.info(`Scheduled backup job: ${name} (${schedule})`);
  }

  async runDatabaseBackup() {
    const script = path.join(__dirname, 'scripts/postgres-backup.sh');
    return this.executeScript(script, {
      DB_HOST: this.config.database.host,
      DB_NAME: this.config.database.name,
      DB_USER: this.config.database.user,
      BACKUP_DIR: this.config.backupDir,
      S3_BUCKET: this.config.s3Bucket
    });
  }

  async runApplicationBackup() {
    const script = path.join(__dirname, 'scripts/application-backup.sh');
    return this.executeScript(script, {
      APP_DIR: this.config.appDir,
      CONFIG_DIR: this.config.configDir,
      BACKUP_DIR: this.config.backupDir,
      S3_BUCKET: this.config.s3Bucket
    });
  }

  async runUploadsBackup() {
    const script = path.join(__dirname, 'scripts/uploads-backup.sh');
    return this.executeScript(script, {
      UPLOADS_DIR: this.config.uploadsDir,
      BACKUP_DIR: this.config.backupDir,
      S3_BUCKET: this.config.s3Bucket
    });
  }

  async runLogsBackup() {
    const logsDir = this.config.logsDir;
    const backupDir = path.join(this.config.backupDir, 'logs');
    const timestamp = new Date().toISOString().slice(0, 10);
    
    await fs.mkdir(backupDir, { recursive: true });
    
    const command = `tar -czf ${backupDir}/logs_${timestamp}.tar.gz -C ${logsDir} .`;
    return this.executeCommand(command);
  }

  async verifyBackups() {
    const backupTypes = ['postgres', 'application', 'uploads', 'logs'];
    const today = new Date().toISOString().slice(0, 10).replace(/-/g, '');
    
    for (const type of backupTypes) {
      try {
        const backupDir = path.join(this.config.backupDir, type);
        const files = await fs.readdir(backupDir);
        
        const todayBackups = files.filter(file => file.includes(today));
        
        if (todayBackups.length === 0) {
          throw new Error(`No backup found for ${type} today`);
        }
        
        // éªŒè¯å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§
        for (const file of todayBackups.slice(0, 1)) { // åªéªŒè¯æœ€æ–°çš„
          const filePath = path.join(backupDir, file);
          await this.verifyBackupFile(filePath);
        }
        
        logger.info(`Backup verification passed: ${type}`);
      } catch (error) {
        logger.error(`Backup verification failed: ${type}`, { error: error.message });
        await this.sendAlertNotification(`backup-verification-${type}`, error);
      }
    }
  }

  async verifyBackupFile(filePath) {
    const stats = await fs.stat(filePath);
    
    if (stats.size === 0) {
      throw new Error('Backup file is empty');
    }
    
    // å¯¹äºå‹ç¼©æ–‡ä»¶ï¼ŒéªŒè¯å‹ç¼©å®Œæ•´æ€§
    if (filePath.endsWith('.gz')) {
      const command = `gunzip -t "${filePath}"`;
      await this.executeCommand(command);
    } else if (filePath.endsWith('.tar.gz')) {
      const command = `tar -tzf "${filePath}" > /dev/null`;
      await this.executeCommand(command);
    }
  }

  async executeScript(scriptPath, env = {}) {
    const envVars = { ...process.env, ...env };
    return this.executeCommand(`bash "${scriptPath}"`, envVars);
  }

  async executeCommand(command, env = process.env) {
    return new Promise((resolve, reject) => {
      exec(command, { env }, (error, stdout, stderr) => {
        if (error) {
          logger.error(`Command failed: ${command}`, { 
            error: error.message,
            stderr 
          });
          reject(error);
        } else {
          logger.info(`Command completed: ${command}`, { stdout });
          resolve(stdout);
        }
      });
    });
  }

  async sendAlertNotification(jobName, error) {
    // å‘é€å‘Šè­¦é€šçŸ¥ï¼ˆSlack, Emailç­‰ï¼‰
    const alertMessage = {
      title: `Backup Job Failed: ${jobName}`,
      message: error.message,
      timestamp: new Date().toISOString(),
      severity: 'critical'
    };
    
    // è¿™é‡Œå¯ä»¥é›†æˆå„ç§é€šçŸ¥æ¸ é“
    logger.error('Backup alert', alertMessage);
  }

  stop() {
    logger.info('Stopping backup scheduler');
    
    for (const [name, job] of this.backupJobs) {
      job.stop();
      logger.info(`Stopped backup job: ${name}`);
    }
    
    this.backupJobs.clear();
  }

  // æ‰‹åŠ¨è§¦å‘å¤‡ä»½
  async triggerBackup(jobName) {
    const jobMap = {
      'database': () => this.runDatabaseBackup(),
      'application': () => this.runApplicationBackup(),
      'uploads': () => this.runUploadsBackup(),
      'logs': () => this.runLogsBackup()
    };
    
    const job = jobMap[jobName];
    if (!job) {
      throw new Error(`Unknown backup job: ${jobName}`);
    }
    
    logger.info(`Manually triggering backup: ${jobName}`);
    await job();
    logger.info(`Manual backup completed: ${jobName}`);
  }
}

module.exports = BackupScheduler;
```

### å¤‡ä»½é…ç½®

```javascript
// backup-config.js
const config = {
  // åŸºç¡€é…ç½®
  timezone: 'Asia/Shanghai',
  backupDir: '/backups',
  
  // æ•°æ®åº“é…ç½®
  database: {
    host: process.env.DB_HOST || 'localhost',
    port: process.env.DB_PORT || 5432,
    name: process.env.DB_NAME || 'nodejs_app',
    user: process.env.DB_USER || 'postgres'
  },
  
  // åº”ç”¨é…ç½®
  appDir: '/opt/nodejs-app',
  configDir: '/etc/nodejs-app',
  uploadsDir: '/var/www/uploads',
  logsDir: '/var/log/nodejs-app',
  
  // äº‘å­˜å‚¨é…ç½®
  s3Bucket: process.env.S3_BACKUP_BUCKET,
  
  // ä¿ç•™ç­–ç•¥
  retention: {
    daily: 30,    // ä¿ç•™30å¤©çš„æ¯æ—¥å¤‡ä»½
    weekly: 12,   // ä¿ç•™12å‘¨çš„æ¯å‘¨å¤‡ä»½
    monthly: 12   // ä¿ç•™12ä¸ªæœˆçš„æ¯æœˆå¤‡ä»½
  },
  
  // å¤‡ä»½éªŒè¯
  verification: {
    enabled: true,
    maxAge: 24 * 60 * 60 * 1000, // 24å°æ—¶å†…çš„å¤‡ä»½æ‰éªŒè¯
    sampleSize: 1 // æ¯ç§ç±»å‹éªŒè¯æœ€æ–°çš„1ä¸ªå¤‡ä»½
  },
  
  // é€šçŸ¥é…ç½®
  notifications: {
    slack: {
      enabled: process.env.SLACK_WEBHOOK_URL ? true : false,
      webhookUrl: process.env.SLACK_WEBHOOK_URL,
      channel: '#alerts'
    },
    email: {
      enabled: process.env.SMTP_HOST ? true : false,
      recipients: ['admin@company.com', 'devops@company.com']
    }
  }
};

module.exports = config;
```

## ğŸ”„ ç¾éš¾æ¢å¤è®¡åˆ’

### æ•°æ®åº“æ¢å¤

```bash
#!/bin/bash
# postgres-restore.sh

set -e

# é…ç½®å˜é‡
BACKUP_FILE=${1:-""}
DB_HOST=${DB_HOST:-localhost}
DB_PORT=${DB_PORT:-5432}
DB_NAME=${DB_NAME:-nodejs_app}
DB_USER=${DB_USER:-postgres}

if [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <backup_file>"
    exit 1
fi

echo "ğŸ”„ å¼€å§‹æ•°æ®åº“æ¢å¤"
echo "å¤‡ä»½æ–‡ä»¶: $BACKUP_FILE"
echo "ç›®æ ‡æ•°æ®åº“: $DB_NAME @ $DB_HOST:$DB_PORT"

# ç¡®è®¤æ“ä½œ
read -p "âš ï¸  è¿™å°†è¦†ç›–ç°æœ‰æ•°æ®åº“ã€‚ç¡®è®¤ç»§ç»­ï¼Ÿ(yes/no): " confirm
if [ "$confirm" != "yes" ]; then
    echo "æ“ä½œå·²å–æ¶ˆ"
    exit 0
fi

# è§£å‹å¤‡ä»½æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
if [[ "$BACKUP_FILE" == *.gz ]]; then
    echo "ğŸ“¦ è§£å‹å¤‡ä»½æ–‡ä»¶..."
    TEMP_FILE="/tmp/$(basename $BACKUP_FILE .gz)"
    gunzip -c "$BACKUP_FILE" > "$TEMP_FILE"
    BACKUP_FILE="$TEMP_FILE"
fi

# åœæ­¢åº”ç”¨æœåŠ¡
echo "â¸ï¸  åœæ­¢åº”ç”¨æœåŠ¡..."
systemctl stop nodejs-app || true

# åˆ›å»ºæ¢å¤å‰å¤‡ä»½
echo "ğŸ’¾ åˆ›å»ºæ¢å¤å‰å¤‡ä»½..."
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
pg_dump \
    --host="$DB_HOST" \
    --port="$DB_PORT" \
    --username="$DB_USER" \
    --dbname="$DB_NAME" \
    --file="/tmp/pre_restore_backup_${TIMESTAMP}.sql" \
    || echo "âš ï¸  æ— æ³•åˆ›å»ºæ¢å¤å‰å¤‡ä»½ï¼Œå¯èƒ½æ•°æ®åº“ä¸å­˜åœ¨"

# æ‰§è¡Œæ¢å¤
echo "ğŸ”„ æ‰§è¡Œæ•°æ®åº“æ¢å¤..."
psql \
    --host="$DB_HOST" \
    --port="$DB_PORT" \
    --username="$DB_USER" \
    --dbname="postgres" \
    --file="$BACKUP_FILE"

# éªŒè¯æ¢å¤
echo "ğŸ” éªŒè¯æ•°æ®åº“æ¢å¤..."
TABLE_COUNT=$(psql \
    --host="$DB_HOST" \
    --port="$DB_PORT" \
    --username="$DB_USER" \
    --dbname="$DB_NAME" \
    --tuples-only \
    --command="SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';" \
    | xargs)

echo "âœ… æ¢å¤å®Œæˆï¼Œå…± $TABLE_COUNT ä¸ªè¡¨"

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
if [ -f "$TEMP_FILE" ]; then
    rm -f "$TEMP_FILE"
fi

# é‡å¯åº”ç”¨æœåŠ¡
echo "â–¶ï¸  é‡å¯åº”ç”¨æœåŠ¡..."
systemctl start nodejs-app

echo "ğŸ‰ æ•°æ®åº“æ¢å¤å®Œæˆ"
```

### å®Œæ•´ç³»ç»Ÿæ¢å¤

```bash
#!/bin/bash
# system-restore.sh

set -e

# é…ç½®å˜é‡
BACKUP_DATE=${1:-""}
BACKUP_DIR=${BACKUP_DIR:-/backups}
S3_BUCKET=${S3_BUCKET:-""}

if [ -z "$BACKUP_DATE" ]; then
    echo "Usage: $0 <backup_date> (format: YYYYMMDD)"
    exit 1
fi

echo "ğŸ”„ å¼€å§‹ç³»ç»Ÿå®Œæ•´æ¢å¤"
echo "æ¢å¤æ—¥æœŸ: $BACKUP_DATE"

# ç¡®è®¤æ“ä½œ
read -p "âš ï¸  è¿™å°†æ¢å¤æ•´ä¸ªç³»ç»Ÿåˆ°æŒ‡å®šæ—¥æœŸçŠ¶æ€ã€‚ç¡®è®¤ç»§ç»­ï¼Ÿ(yes/no): " confirm
if [ "$confirm" != "yes" ]; then
    echo "æ“ä½œå·²å–æ¶ˆ"
    exit 0
fi

# ä»S3ä¸‹è½½å¤‡ä»½ï¼ˆå¦‚æœéœ€è¦ï¼‰
if [ -n "$S3_BUCKET" ]; then
    echo "â˜ï¸ ä»S3ä¸‹è½½å¤‡ä»½æ–‡ä»¶..."
    
    aws s3 cp "s3://$S3_BUCKET/postgres/nodejs_app_${BACKUP_DATE}_*.sql.gz" "$BACKUP_DIR/postgres/" || true
    aws s3 cp "s3://$S3_BUCKET/application/app_backup_${BACKUP_DATE}_*.tar.gz" "$BACKUP_DIR/application/" || true
    aws s3 cp "s3://$S3_BUCKET/uploads/uploads_${BACKUP_DATE}_*.tar.gz" "$BACKUP_DIR/uploads/" || true
fi

# åœæ­¢æ‰€æœ‰æœåŠ¡
echo "â¸ï¸  åœæ­¢æ‰€æœ‰æœåŠ¡..."
systemctl stop nodejs-app
systemctl stop nginx
systemctl stop postgresql

# æ¢å¤æ•°æ®åº“
echo "ğŸ—„ï¸  æ¢å¤æ•°æ®åº“..."
DB_BACKUP=$(find "$BACKUP_DIR/postgres" -name "nodejs_app_${BACKUP_DATE}_*.sql.gz" | head -1)
if [ -n "$DB_BACKUP" ]; then
    ./postgres-restore.sh "$DB_BACKUP"
else
    echo "âŒ æœªæ‰¾åˆ°æ•°æ®åº“å¤‡ä»½æ–‡ä»¶"
    exit 1
fi

# æ¢å¤åº”ç”¨ä»£ç 
echo "ğŸ“ æ¢å¤åº”ç”¨ä»£ç ..."
APP_BACKUP=$(find "$BACKUP_DIR/application" -name "app_backup_${BACKUP_DATE}_*.tar.gz" | head -1)
if [ -n "$APP_BACKUP" ]; then
    # å¤‡ä»½å½“å‰åº”ç”¨
    mv /opt/nodejs-app /opt/nodejs-app.backup.$(date +%s)
    
    # è§£å‹å¤‡ä»½
    tar -xzf "$APP_BACKUP" -C /
    
    # é‡æ–°å®‰è£…ä¾èµ–
    cd /opt/nodejs-app
    npm ci --only=production
else
    echo "âŒ æœªæ‰¾åˆ°åº”ç”¨å¤‡ä»½æ–‡ä»¶"
fi

# æ¢å¤ç”¨æˆ·æ–‡ä»¶
echo "ğŸ“ æ¢å¤ç”¨æˆ·æ–‡ä»¶..."
UPLOADS_BACKUP=$(find "$BACKUP_DIR/uploads" -name "uploads_${BACKUP_DATE}_*.tar.gz" | head -1)
if [ -n "$UPLOADS_BACKUP" ]; then
    # å¤‡ä»½å½“å‰æ–‡ä»¶
    mv /var/www/uploads /var/www/uploads.backup.$(date +%s)
    
    # è§£å‹å¤‡ä»½
    mkdir -p /var/www/uploads
    tar -xzf "$UPLOADS_BACKUP" -C /var/www/uploads
    
    # è®¾ç½®æƒé™
    chown -R www-data:www-data /var/www/uploads
else
    echo "âš ï¸  æœªæ‰¾åˆ°ç”¨æˆ·æ–‡ä»¶å¤‡ä»½"
fi

# å¯åŠ¨æœåŠ¡
echo "â–¶ï¸  å¯åŠ¨æœåŠ¡..."
systemctl start postgresql
systemctl start nodejs-app
systemctl start nginx

# éªŒè¯æ¢å¤
echo "ğŸ” éªŒè¯ç³»ç»Ÿæ¢å¤..."
sleep 10

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
if systemctl is-active --quiet nodejs-app; then
    echo "âœ… Node.jsåº”ç”¨è¿è¡Œæ­£å¸¸"
else
    echo "âŒ Node.jsåº”ç”¨å¯åŠ¨å¤±è´¥"
    systemctl status nodejs-app
fi

# æ£€æŸ¥HTTPå“åº”
if curl -f http://localhost/health > /dev/null 2>&1; then
    echo "âœ… HTTPæœåŠ¡å“åº”æ­£å¸¸"
else
    echo "âŒ HTTPæœåŠ¡æ— å“åº”"
fi

echo "ğŸ‰ ç³»ç»Ÿæ¢å¤å®Œæˆ"
echo "ğŸ“ è¯·æ£€æŸ¥åº”ç”¨åŠŸèƒ½å¹¶éªŒè¯æ•°æ®å®Œæ•´æ€§"
```

## ğŸ“Š å¤‡ä»½ç›‘æ§å’ŒæŠ¥å‘Š

### å¤‡ä»½çŠ¶æ€ç›‘æ§

```javascript
// backup-monitor.js
const fs = require('fs').promises;
const path = require('path');
const logger = require('./logger');

class BackupMonitor {
  constructor(config) {
    this.config = config;
    this.backupDir = config.backupDir;
  }

  async generateBackupReport() {
    const report = {
      timestamp: new Date().toISOString(),
      backupTypes: {},
      summary: {
        totalBackups: 0,
        totalSize: 0,
        oldestBackup: null,
        newestBackup: null,
        healthStatus: 'healthy'
      }
    };

    const backupTypes = ['postgres', 'application', 'uploads', 'logs'];
    
    for (const type of backupTypes) {
      try {
        const typeReport = await this.analyzeBackupType(type);
        report.backupTypes[type] = typeReport;
        report.summary.totalBackups += typeReport.count;
        report.summary.totalSize += typeReport.totalSize;
      } catch (error) {
        logger.error(`Failed to analyze backup type: ${type}`, { error: error.message });
        report.backupTypes[type] = { error: error.message };
        report.summary.healthStatus = 'unhealthy';
      }
    }

    // æ£€æŸ¥å¤‡ä»½å¥åº·çŠ¶æ€
    await this.checkBackupHealth(report);
    
    return report;
  }

  async analyzeBackupType(type) {
    const typeDir = path.join(this.backupDir, type);
    
    try {
      await fs.access(typeDir);
    } catch {
      return {
        count: 0,
        totalSize: 0,
        files: [],
        status: 'missing_directory'
      };
    }

    const files = await fs.readdir(typeDir);
    const backupFiles = files.filter(file => 
      file.endsWith('.gz') || file.endsWith('.tar.gz') || file.endsWith('.sql')
    );

    const fileDetails = [];
    let totalSize = 0;

    for (const file of backupFiles) {
      const filePath = path.join(typeDir, file);
      const stats = await fs.stat(filePath);
      
      fileDetails.push({
        name: file,
        size: stats.size,
        created: stats.birthtime,
        modified: stats.mtime,
        age: Date.now() - stats.mtime.getTime()
      });
      
      totalSize += stats.size;
    }

    // æŒ‰åˆ›å»ºæ—¶é—´æ’åº
    fileDetails.sort((a, b) => b.created - a.created);

    return {
      count: fileDetails.length,
      totalSize,
      files: fileDetails,
      status: this.getBackupTypeStatus(fileDetails, type)
    };
  }

  getBackupTypeStatus(files, type) {
    if (files.length === 0) {
      return 'no_backups';
    }

    const latestBackup = files[0];
    const maxAge = this.getMaxAgeForType(type);
    
    if (latestBackup.age > maxAge) {
      return 'outdated';
    }

    // æ£€æŸ¥æ˜¯å¦æœ‰æŸåçš„å¤‡ä»½æ–‡ä»¶
    const corruptedFiles = files.filter(file => file.size < 1024); // å°äº1KBå¯èƒ½æŸå
    if (corruptedFiles.length > 0) {
      return 'corrupted_files';
    }

    return 'healthy';
  }

  getMaxAgeForType(type) {
    const maxAges = {
      postgres: 24 * 60 * 60 * 1000,    // 24å°æ—¶
      application: 24 * 60 * 60 * 1000,  // 24å°æ—¶
      uploads: 6 * 60 * 60 * 1000,       // 6å°æ—¶
      logs: 24 * 60 * 60 * 1000          // 24å°æ—¶
    };
    
    return maxAges[type] || 24 * 60 * 60 * 1000;
  }

  async checkBackupHealth(report) {
    const issues = [];

    // æ£€æŸ¥å„ä¸ªå¤‡ä»½ç±»å‹çš„å¥åº·çŠ¶æ€
    for (const [type, data] of Object.entries(report.backupTypes)) {
      if (data.error) {
        issues.push(`${type}: ${data.error}`);
      } else if (data.status !== 'healthy') {
        issues.push(`${type}: ${data.status}`);
      }
    }

    // æ£€æŸ¥æ€»ä½“å¤‡ä»½æ•°é‡
    if (report.summary.totalBackups === 0) {
      issues.push('No backups found');
      report.summary.healthStatus = 'critical';
    } else if (issues.length > 0) {
      report.summary.healthStatus = 'warning';
    }

    report.summary.issues = issues;
  }

  async cleanupOldBackups() {
    const retention = this.config.retention;
    const results = {
      cleaned: {},
      errors: {}
    };

    const backupTypes = ['postgres', 'application', 'uploads', 'logs'];
    
    for (const type of backupTypes) {
      try {
        const cleaned = await this.cleanupBackupType(type, retention.daily);
        results.cleaned[type] = cleaned;
        logger.info(`Cleaned up old backups for ${type}`, { count: cleaned });
      } catch (error) {
        results.errors[type] = error.message;
        logger.error(`Failed to cleanup backups for ${type}`, { error: error.message });
      }
    }

    return results;
  }

  async cleanupBackupType(type, retentionDays) {
    const typeDir = path.join(this.backupDir, type);
    const cutoffDate = new Date(Date.now() - retentionDays * 24 * 60 * 60 * 1000);
    
    try {
      const files = await fs.readdir(typeDir);
      let cleanedCount = 0;

      for (const file of files) {
        const filePath = path.join(typeDir, file);
        const stats = await fs.stat(filePath);
        
        if (stats.mtime < cutoffDate) {
          await fs.unlink(filePath);
          cleanedCount++;
          logger.info(`Deleted old backup: ${filePath}`);
        }
      }

      return cleanedCount;
    } catch (error) {
      if (error.code === 'ENOENT') {
        return 0; // ç›®å½•ä¸å­˜åœ¨ï¼Œè¿”å›0
      }
      throw error;
    }
  }

  formatSize(bytes) {
    const sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB'];
    if (bytes === 0) return '0 Bytes';
    const i = Math.floor(Math.log(bytes) / Math.log(1024));
    return Math.round(bytes / Math.pow(1024, i) * 100) / 100 + ' ' + sizes[i];
  }

  async generateHtmlReport(report) {
    const html = `
    <!DOCTYPE html>
    <html>
    <head>
        <title>Backup Report - ${new Date(report.timestamp).toLocaleDateString()}</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            .header { background-color: #f4f4f4; padding: 20px; border-radius: 5px; }
            .summary { display: flex; justify-content: space-between; margin: 20px 0; }
            .metric { text-align: center; }
            .status-healthy { color: green; }
            .status-warning { color: orange; }
            .status-critical { color: red; }
            table { width: 100%; border-collapse: collapse; margin: 20px 0; }
            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
            th { background-color: #f2f2f2; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>Backup Report</h1>
            <p>Generated: ${new Date(report.timestamp).toLocaleString()}</p>
            <p>Status: <span class="status-${report.summary.healthStatus}">${report.summary.healthStatus.toUpperCase()}</span></p>
        </div>
        
        <div class="summary">
            <div class="metric">
                <h3>${report.summary.totalBackups}</h3>
                <p>Total Backups</p>
            </div>
            <div class="metric">
                <h3>${this.formatSize(report.summary.totalSize)}</h3>
                <p>Total Size</p>
            </div>
        </div>
        
        ${Object.entries(report.backupTypes).map(([type, data]) => `
            <h2>${type.charAt(0).toUpperCase() + type.slice(1)} Backups</h2>
            <p>Status: <span class="status-${data.status || 'unknown'}">${(data.status || 'unknown').toUpperCase()}</span></p>
            <p>Count: ${data.count || 0}</p>
            <p>Total Size: ${this.formatSize(data.totalSize || 0)}</p>
            
            ${data.files && data.files.length > 0 ? `
                <table>
                    <tr>
                        <th>File</th>
                        <th>Size</th>
                        <th>Created</th>
                        <th>Age</th>
                    </tr>
                    ${data.files.slice(0, 10).map(file => `
                        <tr>
                            <td>${file.name}</td>
                            <td>${this.formatSize(file.size)}</td>
                            <td>${new Date(file.created).toLocaleString()}</td>
                            <td>${Math.round(file.age / (1000 * 60 * 60))} hours</td>
                        </tr>
                    `).join('')}
                </table>
            ` : '<p>No backup files found</p>'}
        `).join('')}
        
        ${report.summary.issues && report.summary.issues.length > 0 ? `
            <h2>Issues</h2>
            <ul>
                ${report.summary.issues.map(issue => `<li>${issue}</li>`).join('')}
            </ul>
        ` : ''}
    </body>
    </html>
    `;
    
    return html;
  }
}

module.exports = BackupMonitor;
```

## ğŸ“ æ€»ç»“

æœ‰æ•ˆçš„å¤‡ä»½ç­–ç•¥åº”è¯¥åŒ…æ‹¬ï¼š

- **å¤šå±‚æ¬¡å¤‡ä»½**ï¼šæ•°æ®åº“ã€åº”ç”¨ã€é…ç½®ã€ç”¨æˆ·æ–‡ä»¶
- **è‡ªåŠ¨åŒ–è°ƒåº¦**ï¼šå®šæœŸæ‰§è¡Œå¤‡ä»½ä»»åŠ¡
- **å¼‚åœ°å­˜å‚¨**ï¼šäº‘å­˜å‚¨æˆ–è¿œç¨‹å¤‡ä»½
- **å¤‡ä»½éªŒè¯**ï¼šç¡®ä¿å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§
- **å¿«é€Ÿæ¢å¤**ï¼šåˆ¶å®šæ˜ç¡®çš„æ¢å¤æµç¨‹
- **ç›‘æ§æŠ¥å‘Š**ï¼šåŠæ—¶å‘ç°å¤‡ä»½é—®é¢˜

å¤‡ä»½ç­–ç•¥æ˜¯ä¸šåŠ¡è¿ç»­æ€§çš„é‡è¦ä¿éšœï¼Œéœ€è¦å®šæœŸæµ‹è¯•å’Œä¼˜åŒ–ã€‚

## ğŸ”— ç›¸å…³èµ„æº

- [PostgreSQLå¤‡ä»½æ–‡æ¡£](https://www.postgresql.org/docs/current/backup.html)
- [MongoDBå¤‡ä»½æŒ‡å—](https://docs.mongodb.com/manual/core/backups/)
- [AWSå¤‡ä»½æœåŠ¡](https://aws.amazon.com/backup/)
- [å¤‡ä»½æœ€ä½³å®è·µ](https://www.veeam.com/blog/321-backup-rule.html)
